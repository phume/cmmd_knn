\documentclass[conference]{IEEEtran}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{PNKIF: Peer-Normalized Kernel Isolation Forest for Training-Free Contextual Anomaly Detection}

\author{
\IEEEauthorblockN{Author Name}
\IEEEauthorblockA{Affiliation\\
Email}
}

\begin{document}

\maketitle

\begin{abstract}
When should anomaly detection condition on context? Standard methods like Isolation Forest detect \emph{global} anomalies---samples unusual everywhere. But some anomalies are \emph{contextual}: normal globally, yet unusual for their specific context (e.g., a domestic bank account with cross-border transaction patterns). We propose Peer-Normalized Kernel Isolation Forest (PNKIF), a diagnostic tool that activates precisely when context matters. PNKIF identifies contextually similar peers via K-nearest neighbors, normalizes behavior against peer statistics, then applies Isolation Forest. On datasets with true contextual anomalies, PNKIF achieves $0.95 \pm 0.03$ AUROC where standard IF scores $0.50 \pm 0.01$ (random). On datasets with global anomalies, IF matches or exceeds PNKIF---as expected, since context-conditioning provides no benefit. This diagnostic property is valuable: practitioners can run both methods, and divergence signals that contextual structure exists. We demonstrate on an anti-money laundering dataset (293K accounts) where injecting contextual violations (domestic accounts with cross-border behavior) causes IF to drop from $0.96$ to $0.90$ while PNKIF rises to $0.95$. The method scales as $O(N \log N)$ and requires no training.
\end{abstract}

\input{sections/introduction}
\input{sections/related_work}
\input{sections/methodology}
\input{sections/experiments}
\input{sections/discussion}
\input{sections/conclusion}

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
