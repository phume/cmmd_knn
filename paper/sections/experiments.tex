\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

\subsubsection{Datasets}
We evaluate on 11 datasets spanning three categories:

\begin{itemize}
    \item \textbf{Synthetic}: Controlled datasets with true contextual anomalies---Syn-Cluster (cluster-specific behavior) and Syn-HighDim (20D context with only 2 informative dimensions).
    \item \textbf{Real + Injection}: Real datasets with injected contextual anomalies---Adult and Bank (UCI) with demographic-behavior swaps, and SAML-D with domestic-to-cross-border behavior injection.
    \item \textbf{Real (Original)}: Fraud detection benchmarks with natural anomalies---Cardio, Thyroid, SAML-D, PaySim, IEEE-CIS, and CreditCard.
\end{itemize}

\textbf{Why injection is necessary}: Standard benchmarks contain predominantly \emph{global} anomalies that are unusual everywhere. Without controlled contextual violations, CAD methods cannot be meaningfully evaluated---global methods would appear equivalent to contextual methods. Injection creates anomalies that are normal globally but unusual for their context, isolating the property we wish to test.

\textbf{What injection does NOT assume}: We make no assumptions about (1) fraud labels or ground truth semantics, (2) whether injected patterns represent real crimes, or (3) the operational meaning of anomalies. Injection tests a purely \emph{statistical} property: can the method detect samples whose behavior deviates from their context group's distribution? This is the defining capability of contextual anomaly detection, independent of downstream interpretation.

\begin{table}[h]
\centering
\caption{Dataset statistics. $d_c$: context dimensions, $d_y$: behavior dimensions.}
\label{tab:datasets}
\small
\begin{tabular}{@{}lrrrr@{}}
\toprule
Dataset & $N$ & $d_c$ & $d_y$ & Anom.\% \\
\midrule
\multicolumn{5}{l}{\textit{Synthetic}} \\
Syn-Cluster & 10,000 & 2 & 3 & 5.0\% \\
Syn-HighDim & 10,000 & 20 & 3 & 5.0\% \\
\midrule
\multicolumn{5}{l}{\textit{Real + Injection}} \\
Adult & 30,162 & 5 & 4 & 5.0\% \\
Bank & 30,488 & 7 & 9 & 5.0\% \\
SAML-D (inj) & 292,715 & 38 & 11 & 3.7\% \\
\midrule
\multicolumn{5}{l}{\textit{Real (Original)}} \\
Cardio & 1,831 & 5 & 16 & 9.6\% \\
Thyroid & 3,772 & 3 & 3 & 2.5\% \\
SAML-D & 292,715 & 38 & 11 & 1.7\% \\
PaySim & 50,000 & 3 & 7 & 0.2\% \\
IEEE-CIS & 50,000 & 11 & 31 & 2.7\% \\
CreditCard & 284,807 & 2 & 28 & 0.17\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Baselines}
We compare against 7 baseline methods:
\begin{itemize}
    \item \textbf{IF}: Isolation Forest on behavioral features only~\cite{liu2008isolation}.
    \item \textbf{IF$_c$}: IF on concatenated context and behavior.
    \item \textbf{DIF}: Deep Isolation Forest with random MLP projections~\cite{xu2023deep}.
    \item \textbf{DIF$_c$}: DIF on concatenated features.
    \item \textbf{LOF}: Local Outlier Factor on concatenated features~\cite{breunig2000lof}.
    \item \textbf{QCAD}: Quantile-based conditional anomaly detection~\cite{liang2021conditional}.
    \item \textbf{ROCOD}: Robust conditional outlier detection~\cite{liang2023rocod}.
\end{itemize}

We also evaluate \textbf{PNKDIF}, which adds random MLP projections before Isolation Forest scoring, to test whether non-linear feature interactions improve performance.

\subsubsection{Implementation}
For PNKDIF/PNKIF, we use $K=\min(100, N/20)$ neighbors, RBF kernel with median bandwidth heuristic, 6 random projections with 128 hidden dimensions, and 100 isolation trees. All experiments use 10 random seeds; we report mean AUROC.

\subsection{Results}

\input{generated_tables}

Table~\ref{tab:main_results} presents the comprehensive AUROC comparison. The results reveal a clear pattern: the optimal method depends on whether anomalies are \emph{contextual} (unusual for their context) or \emph{global} (unusual everywhere).

\subsubsection{Contextual Anomalies: PNKIF/PNKDIF Excel}

On datasets with true contextual anomalies, PNKIF and PNKDIF dramatically outperform baselines:

\textbf{Syn-Cluster}: PNKDIF achieves 0.953 AUROC while IF scores only 0.504 (random). This 90\% improvement demonstrates that peer normalization is essential when anomalies are defined relative to cluster-specific behavior norms.

\textbf{SAML-D (injected)}: After injecting contextual anomalies (domestic accounts with cross-border transaction patterns), PNKIF achieves 0.948 compared to IF's 0.901. Notably, DIF drops to 0.750, showing that ignoring context hurts performance on contextual anomalies.

\textbf{Key insight}: The SAML-D comparison (with vs. without injection) is particularly informative. On original SAML-D labels, IF achieves 0.961 (best). After injection, IF drops to 0.901 while PNKIF rises to 0.948. This confirms that PNKIF specifically detects contextual anomalies that IF misses.

\subsubsection{Global Anomalies: IF/DIF Suffice}

On datasets where anomalies have global signatures, traditional methods perform well:

\textbf{Cardio \& Thyroid}: IF and IF$_c$ achieve top performance (0.946 and 0.977), while PNKIF/PNKDIF underperform (0.78 and 0.70). These ODDS benchmark anomalies appear to be globally distinguishable without requiring peer comparison.

\textbf{CreditCard}: DIF variants achieve the best results (0.952). With only 2 context dimensions (Time, Amount), context-conditioning provides limited benefit.

\subsubsection{Do MLP Projections Help?}

Comparing PNKIF (no MLP) to PNKDIF (with random MLP projections):
\begin{itemize}
    \item PNKIF wins on 8/11 datasets (average $+1.0\%$ over PNKDIF)
    \item PNKDIF wins only on Syn-HighDim ($+0.9\%$) and Syn-Cluster ($+0.1\%$)
    \item Differences are small ($<2\%$); both dramatically outperform baselines on contextual anomalies
\end{itemize}

The MLP projections provide marginal benefit on high-dimensional synthetic data but slightly hurt performance on real data. \textbf{Conclusion}: The ``Deep'' component is unnecessary---peer normalization alone drives the improvement. We recommend PNKIF for its simplicity and speed.

\subsection{Ablation: Peer Normalization is Essential}

The key component is peer normalization. Without it (using global statistics), PNKDIF reduces to approximately DIF:
\begin{itemize}
    \item On Syn-Cluster: 0.953 $\rightarrow$ 0.506 (near random)
    \item On SAML-D (inj): 0.940 $\rightarrow$ 0.750 (matches DIF)
\end{itemize}

This confirms that the peer-based approach---not the MLP projections---drives the improvement on contextual anomalies.

\subsection{Scalability}

PNKDIF scales as $O(N \log N)$ due to the K-NN step using ball-tree indexing. On SAML-D (293K samples), PNKIF processes the full dataset in approximately 3 minutes. The overhead versus DIF is ~25\%, primarily from neighbor search.

\subsection{Summary}

\begin{itemize}
    \item \textbf{Use PNKIF} when anomalies are contextual (behavior unusual for the specific context)
    \item \textbf{Use IF/DIF} when anomalies are global (unusual everywhere)
    \item MLP projections (PNKDIF) provide marginal benefit; simple peer normalization is sufficient
    \item Real-world applicability demonstrated on SAML-D: detecting accounts with behavior patterns inconsistent with their geographic profile
\end{itemize}
