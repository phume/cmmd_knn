\section{Methodology}
\label{sec:method}

We present Peer-Normalized Kernel Isolation Forest (PNKIF), a training-free contextual anomaly detection method. Our approach combines peer-based normalization with Isolation Forest scoring. We also describe PNKDIF, a variant that adds random MLP projections, though experiments show this provides marginal benefit.

\subsection{Problem Formulation}

Let $\mathcal{X} = \{x_1, \ldots, x_N\}$ denote a dataset where each sample $x_i = (c_i, y_i)$ consists of:
\begin{itemize}
    \item \textbf{Context features} $c_i \in \mathbb{R}^{d_c}$: attributes that define the operating conditions or entity characteristics (e.g., customer demographics, device specifications)
    \item \textbf{Behavioral features} $y_i \in \mathbb{R}^{d_y}$: attributes representing actions or outcomes to be evaluated for anomalousness (e.g., transaction amounts, sensor readings)
\end{itemize}

The goal is to compute an anomaly score $s_i \in [0, 100]$ for each sample, where higher scores indicate greater deviation from contextually similar peers.

\subsection{Algorithm Overview}

PNKIF proceeds through three main stages:
\begin{enumerate}
    \item \textbf{Peer Selection}: For each point, identify $K$ nearest neighbors in context space
    \item \textbf{Peer Normalization}: Compute kernel-weighted peer statistics and z-score normalize behavioral features
    \item \textbf{Isolation Scoring}: Run Isolation Forest on the normalized features
\end{enumerate}

The variant PNKDIF adds a fourth step: applying $M$ frozen randomly-initialized MLPs before Isolation Forest. However, as we show in Section~\ref{sec:experiments}, this provides marginal benefit ($<2\%$) on most datasets.

\subsection{Peer Selection and Kernel Weighting}

For each sample $x_i$, we identify its peer group as the $K$ nearest neighbors in context space, excluding the sample itself:
\begin{equation}
    \mathcal{N}_K(i) = \text{KNN}(c_i, K) \setminus \{i\}
\end{equation}

Rather than treating all peers uniformly, we apply RBF kernel weighting to give closer neighbors greater influence:
\begin{equation}
    w_{ij} = \exp\left(-\frac{\|c_i - c_j\|^2}{2\gamma^2}\right), \quad j \in \mathcal{N}_K(i)
\end{equation}
where $\gamma$ is the kernel bandwidth, typically set via the median heuristic on pairwise distances.

The normalized weights are:
\begin{equation}
    \tilde{w}_{ij} = \frac{w_{ij}}{\sum_{k \in \mathcal{N}_K(i)} w_{ik}}
\end{equation}

\subsection{Peer-Based Normalization}

Using the kernel weights, we compute weighted peer statistics for each sample:
\begin{align}
    \mu_i &= \sum_{j \in \mathcal{N}_K(i)} \tilde{w}_{ij} \cdot y_j \\
    \sigma_i &= \sqrt{\sum_{j \in \mathcal{N}_K(i)} \tilde{w}_{ij} \cdot (y_j - \mu_i)^2}
\end{align}

The behavioral features are then z-score normalized relative to the peer group:
\begin{equation}
    z_i = \frac{y_i - \mu_i}{\max(\sigma_i, \epsilon)}
\end{equation}
where $\epsilon = 10^{-8}$ ensures numerical stability when peers have homogeneous behavior.

This normalization removes context-dependent location and scale effects, allowing downstream anomaly detection to focus on relative deviations.

\subsection{SNN Weighting Variant (PNKIF-SNN)}

Inspired by ConQuest~\cite{calikus2025context}, we also evaluate a variant using Shared Nearest Neighbors (SNN) similarity instead of RBF kernel weighting. SNN is more robust to high-dimensional context spaces where Euclidean distances become less meaningful.

The SNN weight between points $i$ and $j$ is:
\begin{equation}
    w_{ij}^{\text{SNN}} = \frac{|\mathcal{N}_K(i) \cap \mathcal{N}_K(j)|}{K}
\end{equation}
where $\mathcal{N}_K(i)$ denotes the K-nearest neighbors of point $i$. Points with more shared neighbors receive higher weights, capturing local structure better than distance-based metrics in high dimensions.

\subsection{Random MLP Projections (PNKDIF Variant)}

The PNKDIF variant adds random MLP projections to capture potential non-linear anomaly patterns. The normalized features are projected through $M$ randomly-initialized single-layer MLPs:
\begin{equation}
    h_i^{(m)} = \text{LeakyReLU}(z_i W^{(m)} + b^{(m)}), \quad m = 1, \ldots, M
\end{equation}

The weight matrices $W^{(m)} \in \mathbb{R}^{d_y \times d_h}$ are sampled from $\mathcal{N}(0, 2/d_y)$ following Kaiming initialization \cite{he2015delving}, and biases $b^{(m)}$ are set to zero. Critically, these weights are \emph{frozen} after initialization---no training occurs.

This approach is motivated by two observations:
\begin{enumerate}
    \item Random features can approximate kernel methods \cite{rahimi2007random}, providing non-linear decision boundaries without optimization
    \item Multiple random projections create diverse views of the data, where different projections may emphasize different feature interactions
\end{enumerate}

\subsection{Isolation Forest Scoring}

\textbf{PNKIF}: We fit an Isolation Forest \cite{liu2008isolation} directly on the z-score normalized features $\{z_i\}_{i=1}^N$:
\begin{equation}
    s_i = \text{IF}(z_i)
\end{equation}

\textbf{PNKDIF}: For each projection $m$, we fit an Isolation Forest on the MLP-transformed representations $\{h_i^{(m)}\}_{i=1}^N$:
\begin{equation}
    s_i^{(m)} = \text{IF}^{(m)}(h_i^{(m)})
\end{equation}

The Isolation Forest operates by recursively partitioning the feature space with random axis-aligned splits. Points that are isolated quickly (short path length) receive higher anomaly scores.

\subsection{Score Aggregation}

For PNKIF, the Isolation Forest score is used directly (rescaled to $[0, 100]$).

For PNKDIF, the final anomaly score aggregates across all projections:
\begin{equation}
    s_i^{\text{raw}} = \frac{1}{M} \sum_{m=1}^{M} s_i^{(m)}
\end{equation}
\begin{equation}
    s_i = 100 \cdot \frac{s_i^{\text{raw}} - \min_j s_j^{\text{raw}}}{\max_j s_j^{\text{raw}} - \min_j s_j^{\text{raw}}}
\end{equation}

\subsection{Hyperparameters}

Table~\ref{tab:hyperparameters} summarizes the hyperparameters and their typical ranges.

\begin{table}[h]
\centering
\caption{PNKIF/PNKDIF Hyperparameters}
\label{tab:hyperparameters}
\begin{tabular}{@{}llll@{}}
\toprule
Symbol & Description & Typical Range & Variant \\
\midrule
$K$ & Number of neighbors & 50--200 & Both \\
$\gamma$ & RBF kernel bandwidth & Data-dependent & Both \\
$T$ & Number of IF trees & 100--300 & Both \\
$\psi$ & IF subsample size & 256 & Both \\
$M$ & Number of random projections & 6--10 & PNKDIF only \\
$d_h$ & Hidden dimension & 64--256 & PNKDIF only \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Computational Complexity}

The time complexity is dominated by:
\begin{itemize}
    \item KD-tree construction and queries: $O(N \log N \cdot d_c)$
    \item Kernel weights and peer statistics: $O(N \cdot K \cdot (d_c + d_y))$
    \item Isolation Forest scoring: $O(N \cdot T \cdot \log \psi)$
    \item Random projections (PNKDIF only): $O(N \cdot M \cdot d_y \cdot d_h)$
\end{itemize}

For fixed hyperparameters, the overall complexity is $O(N \log N)$, making both PNKIF and PNKDIF scalable to large datasets. PNKIF is faster than PNKDIF by avoiding the projection step. On SAML-D (293K samples), PNKIF processes the full dataset in approximately 3 minutes.
