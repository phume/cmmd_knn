\section{Methodology}
\label{sec:method}

We present Peer-Normalized Kernel Deep Isolation Forest (PNKDIF), a training-free contextual anomaly detection method. Our approach combines peer-based normalization with random non-linear projections and Isolation Forest scoring. This section formalizes each component of the pipeline.

\subsection{Problem Formulation}

Let $\mathcal{X} = \{x_1, \ldots, x_N\}$ denote a dataset where each sample $x_i = (c_i, y_i)$ consists of:
\begin{itemize}
    \item \textbf{Context features} $c_i \in \mathbb{R}^{d_c}$: attributes that define the operating conditions or entity characteristics (e.g., customer demographics, device specifications)
    \item \textbf{Behavioral features} $y_i \in \mathbb{R}^{d_y}$: attributes representing actions or outcomes to be evaluated for anomalousness (e.g., transaction amounts, sensor readings)
\end{itemize}

The goal is to compute an anomaly score $s_i \in [0, 100]$ for each sample, where higher scores indicate greater deviation from contextually similar peers.

\subsection{Algorithm Overview}

PNKDIF proceeds through four main stages:
\begin{enumerate}
    \item \textbf{Peer Selection}: For each point, identify $K$ nearest neighbors in context space
    \item \textbf{Peer Normalization}: Compute kernel-weighted peer statistics and z-score normalize behavioral features
    \item \textbf{Random Projection}: Apply $M$ frozen randomly-initialized MLPs to create diverse non-linear representations
    \item \textbf{Isolation Scoring}: Run Isolation Forest on each representation and aggregate scores
\end{enumerate}

\subsection{Peer Selection and Kernel Weighting}

For each sample $x_i$, we identify its peer group as the $K$ nearest neighbors in context space, excluding the sample itself:
\begin{equation}
    \mathcal{N}_K(i) = \text{KNN}(c_i, K) \setminus \{i\}
\end{equation}

Rather than treating all peers uniformly, we apply RBF kernel weighting to give closer neighbors greater influence:
\begin{equation}
    w_{ij} = \exp\left(-\frac{\|c_i - c_j\|^2}{2\gamma^2}\right), \quad j \in \mathcal{N}_K(i)
\end{equation}
where $\gamma$ is the kernel bandwidth, typically set via the median heuristic on pairwise distances.

The normalized weights are:
\begin{equation}
    \tilde{w}_{ij} = \frac{w_{ij}}{\sum_{k \in \mathcal{N}_K(i)} w_{ik}}
\end{equation}

\subsection{Peer-Based Normalization}

Using the kernel weights, we compute weighted peer statistics for each sample:
\begin{align}
    \mu_i &= \sum_{j \in \mathcal{N}_K(i)} \tilde{w}_{ij} \cdot y_j \\
    \sigma_i &= \sqrt{\sum_{j \in \mathcal{N}_K(i)} \tilde{w}_{ij} \cdot (y_j - \mu_i)^2}
\end{align}

The behavioral features are then z-score normalized relative to the peer group:
\begin{equation}
    z_i = \frac{y_i - \mu_i}{\max(\sigma_i, \epsilon)}
\end{equation}
where $\epsilon = 10^{-8}$ ensures numerical stability when peers have homogeneous behavior.

This normalization removes context-dependent location and scale effects, allowing downstream anomaly detection to focus on relative deviations.

\subsection{Random MLP Projections}

To capture non-linear anomaly patterns, we project the normalized features through $M$ randomly-initialized single-layer MLPs:
\begin{equation}
    h_i^{(m)} = \text{LeakyReLU}(z_i W^{(m)} + b^{(m)}), \quad m = 1, \ldots, M
\end{equation}

The weight matrices $W^{(m)} \in \mathbb{R}^{d_y \times d_h}$ are sampled from $\mathcal{N}(0, 2/d_y)$ following Kaiming initialization \cite{he2015delving}, and biases $b^{(m)}$ are set to zero. Critically, these weights are \emph{frozen} after initialization---no training occurs.

This approach is motivated by two observations:
\begin{enumerate}
    \item Random features can approximate kernel methods \cite{rahimi2007random}, providing non-linear decision boundaries without optimization
    \item Multiple random projections create diverse views of the data, where different projections may emphasize different feature interactions
\end{enumerate}

\subsection{Isolation Forest Scoring}

For each projection $m$, we fit an Isolation Forest \cite{liu2008isolation} on the transformed representations $\{h_i^{(m)}\}_{i=1}^N$:
\begin{equation}
    s_i^{(m)} = \text{IF}^{(m)}(h_i^{(m)})
\end{equation}

The Isolation Forest operates by recursively partitioning the feature space with random axis-aligned splits. Points that are isolated quickly (short path length) receive higher anomaly scores. In the MLP-transformed space, these axis-aligned cuts correspond to non-linear boundaries in the original z-score space.

\subsection{Score Aggregation}

The final anomaly score aggregates across all projections and is rescaled to $[0, 100]$:
\begin{equation}
    s_i^{\text{raw}} = \frac{1}{M} \sum_{m=1}^{M} s_i^{(m)}
\end{equation}
\begin{equation}
    s_i = 100 \cdot \frac{s_i^{\text{raw}} - \min_j s_j^{\text{raw}}}{\max_j s_j^{\text{raw}} - \min_j s_j^{\text{raw}}}
\end{equation}

\subsection{Hyperparameters}

Table~\ref{tab:hyperparameters} summarizes the hyperparameters and their typical ranges.

\begin{table}[h]
\centering
\caption{PNKDIF Hyperparameters}
\label{tab:hyperparameters}
\begin{tabular}{@{}lll@{}}
\toprule
Symbol & Description & Typical Range \\
\midrule
$K$ & Number of neighbors & 50--200 \\
$\gamma$ & RBF kernel bandwidth & Data-dependent \\
$M$ & Number of random projections & 6--10 \\
$d_h$ & Hidden dimension & 64--256 \\
$T$ & Number of IF trees & 100--300 \\
$\psi$ & IF subsample size & 256 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Computational Complexity}

The time complexity of PNKDIF is dominated by:
\begin{itemize}
    \item KD-tree construction and queries: $O(N \log N \cdot d_c)$
    \item Kernel weights and peer statistics: $O(N \cdot K \cdot (d_c + d_y))$
    \item Random projections: $O(N \cdot M \cdot d_y \cdot d_h)$
    \item Isolation Forest scoring: $O(N \cdot M \cdot T \cdot \log \psi)$
\end{itemize}

For fixed hyperparameters, the overall complexity is $O(N \log N)$, making PNKDIF scalable to large datasets. The algorithm is also highly parallelizable: K-NN queries, projections, and IF scoring are independent per sample or per projection.
